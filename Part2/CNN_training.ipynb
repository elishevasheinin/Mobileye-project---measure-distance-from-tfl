{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "Copy of tfl_detection.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmCGa79bCVFQ"
      },
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%autosave 120\n",
        "%matplotlib inline\n",
        "import numpy as np, matplotlib.pyplot as plt\n",
        "from os.path import join"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBoUx11Vdhor"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SnIbYL0CVFW"
      },
      "source": [
        "## Step 1. Validate the data\n",
        "    use the example in the cell below, to verify the TFL patch you've generated is sane.\n",
        "    Things to watch for:\n",
        "    1. You are able to load and vizualize your train and val data, using the functions below.\n",
        "    2. using the vizualization verify  image <--> label correspondence is correct.\n",
        "    3. % Negative vs. Positive examples is aprroximately 50%\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "QUPyHj32CVFX"
      },
      "source": [
        "def load_tfl_data(data_dir, crop_shape=(81,81)):\n",
        "    images = np.memmap(join(data_dir,'data.bin'),mode='r',dtype=np.uint8).reshape([-1]+list(crop_shape) +[3])\n",
        "    labels = np.memmap(join(data_dir,'labels.bin'),mode='r',dtype=np.uint8)\n",
        "    return {'images':images,'labels':labels}\n",
        "\n",
        "def viz_my_data(images,labels, predictions=None, num=(5,5), labels2name= {0:'No TFL',1:'Yes TFL'}):\n",
        "    assert images.shape[0] == labels.shape[0]\n",
        "    assert predictions is None or predictions.shape[0] == images.shape[0]\n",
        "    h = 5\n",
        "    n = num[0]*num[1]\n",
        "    ax = plt.subplots(num[0],num[1],figsize=(h*num[0],h*num[1]),gridspec_kw={'wspace':0.05},squeeze=False,sharex=True,sharey=True)[1]#.flatten()\n",
        "    idxs = np.random.randint(0,images.shape[0],n)\n",
        "    for i,idx in enumerate(idxs):\n",
        "        ax.flatten()[i].imshow(images[idx])\n",
        "        title = labels2name[labels[idx]]\n",
        "        if predictions is not None : title += ' Prediction: {:.2f}'.format(predictions[idx])\n",
        "        ax.flatten()[i].set_title(title)\n",
        "   \n",
        "data_dir = \"/content/drive/MyDrive/Colab Notebooks/data\"\n",
        "datasets = {\n",
        "    'val':load_tfl_data(join(data_dir,'val')),\n",
        "    'train': load_tfl_data(join(data_dir,'train')),\n",
        "    }\n",
        "for k,v in datasets.items():\n",
        "    \n",
        "    print ('{} :  {} 0/1 split {:.1f} %'.format(k,v['images'].shape, np.mean(v['labels']==1)*100))\n",
        "\n",
        "viz_my_data(num=(6,6),**datasets['val'])   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeQNbkKzCVFc"
      },
      "source": [
        "## define the model used for training\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZV1vwUyj3YIX"
      },
      "source": [
        "from tensorflow.keras.applications.resnet50 import ResNet50\n",
        "from keras.models import Model\n",
        "import keras\n",
        "restnet = ResNet50(include_top=False, weights='imagenet', input_shape=(81,81,3))\n",
        "output = restnet.layers[-1].output\n",
        "output = keras.layers.Flatten()(output)\n",
        "restnet = Model(restnet.input, outputs=output)\n",
        "for layer in restnet.layers:\n",
        "    layer.trainable = False\n",
        "restnet.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1EW04DBVbfNY"
      },
      "source": [
        "[n,w,h,c] = [5,5,5,3]\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "ph = tf.compat.v1.placeholder(shape=[None,w,h,c],dtype=tf.float32)\n",
        "model = dropblock(ph,0.1,3)\n",
        "out = sess.run(model,feed_dict={ph:input})\n",
        "print(out[0,:,:,0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc1Pcv0-5znK"
      },
      "source": [
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, InputLayer\n",
        "from keras.models import Sequential\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import regularizers\n",
        "model = Sequential()\n",
        "model.add(restnet)\n",
        "model.add(Dense(128, activation='relu', input_dim=(81,81,3)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(128,activation=('relu'),input_dim=(81,81,3)))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(2, activation='sigmoid'))\n",
        "model.summary()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUWvUO50CVFg"
      },
      "source": [
        "## train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Agk-E70SCVFq"
      },
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.optimizers.schedules import InverseTimeDecay\n",
        "data_dir = '/content/drive/MyDrive/Colab Notebooks/data'\n",
        "datasets = {\n",
        "    'val':load_tfl_data(join(data_dir,'val')),\n",
        "    'train': load_tfl_data(join(data_dir,'train')),\n",
        "    }\n",
        "N_VALIDATION = int(1e3)\n",
        "N_TRAIN = int(1e4)\n",
        "BUFFER_SIZE = int(1e4)\n",
        "BATCH_SIZE = 500\n",
        "STEPS_PER_EPOCH = N_TRAIN//BATCH_SIZE\n",
        "lr_schedule = InverseTimeDecay(\n",
        "  0.001,\n",
        "  decay_steps=STEPS_PER_EPOCH*1000,\n",
        "  decay_rate=1,\n",
        "  staircase=False)\n",
        "model.compile(optimizer=Adam(),loss =sparse_categorical_crossentropy,metrics=['accuracy'])\n",
        "train,val = datasets['train'],datasets['val']\n",
        "#train it, the model uses the 'train' dataset for learning. We evaluate the \"goodness\" of the model, by predicting the label of the images in the val dataset.\n",
        "history=model.fit(train['images'],train['labels'],validation_data=(val['images'],val['labels']),epochs =4)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zjCrXa9CVF0"
      },
      "source": [
        "epochs = history.history\n",
        "epochs['train_accuracy'] = epochs['accuracy']\n",
        "plt.figure(figsize=(10,10))\n",
        "for k in ['train_accuracy','val_accuracy']:\n",
        "    plt.plot(range(len(epochs[k])), epochs[k],label=k)\n",
        "\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrjQkZTHM16z"
      },
      "source": [
        "epochs = history.history\n",
        "epochs['train_loss'] = epochs['loss']\n",
        "plt.figure(figsize=(10,10))\n",
        "for k in ['train_loss','val_loss']:\n",
        "    plt.plot(range(len(epochs[k])), epochs[k],label=k)\n",
        "\n",
        "plt.legend();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRKv07B2CVGF"
      },
      "source": [
        "## evaluate and predict\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5T347oZCVGH"
      },
      "source": [
        "import seaborn as sbn\n",
        "predictions = model.predict(val['images'])\n",
        "sbn.distplot(predictions[:,0]);\n",
        "\n",
        "predicted_label = np.argmax(predictions, axis=-1)\n",
        "print ('accuracy:', np.mean(predicted_label==val['labels']))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "m7H8KtyICVGJ"
      },
      "source": [
        "viz_my_data(num=(6,6),predictions=predictions[:,1],**val);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KlUo6s2CVGM"
      },
      "source": [
        "### Saving the model\n",
        "After we trained our model and made predictions with it, we will now want to save the **architecture** together with its learned **weights** in order for us to be able to use it in the TFL manager.\n",
        "The architecture will be saved as a json, the weights in the h5 format: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OzAujOXFCVGR"
      },
      "source": [
        "model.save(\"/content/drive/MyDrive/Colab Notebooks/model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}